{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新华网爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取新闻详情\n",
    "def get_news_detail(news_url):\n",
    "    response = requests.get(news_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    title = soup.find('h1').text.strip()\n",
    "    content = ' '.join([p.text.strip() for p in soup.find_all('p') if p.text.strip()])\n",
    "\n",
    "    return title, content\n",
    "\n",
    "# 保存为Word文档\n",
    "def save_to_word(file_path, title, content):\n",
    "    doc = docx.Document()\n",
    "    doc.add_heading(title, level=1)\n",
    "    doc.add_paragraph(content)\n",
    "    doc.save(file_path)\n",
    "\n",
    "# 爬取新闻\n",
    "def scrape_news(base_url, categories, save_path, months=3):\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=30*months)\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    for category in categories:\n",
    "        category_path = os.path.join(save_path, category)\n",
    "        if not os.path.exists(category_path):\n",
    "            os.makedirs(category_path)\n",
    "        \n",
    "        category_url = f\"{base_url}/{category}\"\n",
    "        response = requests.get(category_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        news_links = soup.find_all('a', href=True)\n",
    "        \n",
    "        for link in news_links:\n",
    "            news_url = link['href']\n",
    "            # 假设新闻详情页面包含发布日期信息\n",
    "            try:\n",
    "                title, content = get_news_detail(news_url)\n",
    "                # 假设我们可以从详情页面中提取发布日期\n",
    "                pub_date = datetime.strptime(soup.find('time').text.strip(), '%Y-%m-%d')\n",
    "                if start_date <= pub_date <= end_date:\n",
    "                    file_path = os.path.join(category_path, f\"{title}.docx\")\n",
    "                    save_to_word(file_path, title, content)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {news_url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 示例用法\n",
    "# base_url = 'http://www.xinhuanet.com'\n",
    "# categories = ['politics', 'world', 'sports']  # 替换为实际的新闻分类\n",
    "# save_path = 'news_data'  # 保存路径\n",
    "# scrape_news(base_url, categories, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取文档内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取Word文档内容\n",
    "def read_word_doc(file_path):\n",
    "    print('\\n-----------------读取Word文档内容-----------------')\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        full_text.append(paragraph.text)\n",
    "\n",
    "    text = ' '.join(full_text)\n",
    "    print('去除空格和标点符号前的文本长度：', len(text))\n",
    "\n",
    "    text = re.sub(r'\\s+', '', text)  # 去除所有空格\n",
    "    text = re.sub(r'[^\\w]', '', text)  # 去除所有标点符号\n",
    "    print('去除空格和标点符号后的文本长度：', len(text))\n",
    "    print('文本内容:', text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向量表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算文本的TF-IDF向量表示\n",
    "def compute_tfidf_vector(texts):\n",
    "    print('\\n-----------------计算文本的TF-IDF向量表示-----------------')\n",
    "\n",
    "    # 使用 jieba 进行分词\n",
    "    def chinese_tokenizer(text):\n",
    "        return jieba.lcut(text)\n",
    "\n",
    "    # 初始化 TfidfVectorizer，使用自定义的分词器\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        tokenizer=chinese_tokenizer,  # 自定义分词器\n",
    "        stop_words=None,              # 可以提供中文停用词列表\n",
    "        lowercase=False,              # 保持原始大小写\n",
    "        ngram_range=(1, 2)            # 只使用一元组\n",
    "    )\n",
    "    \n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    tfidf_matrix_array = tfidf_matrix.toarray()\n",
    "    print('TF-IDF矩阵的形状：', tfidf_matrix_array.shape)\n",
    "    # 打印特征名称（单词和短语）\n",
    "    print('打印特征名称：', vectorizer.get_feature_names_out())\n",
    "\n",
    "    return tfidf_matrix_array, vectorizer\n",
    "\n",
    "# 计算文本的Word2Vec向量表示\n",
    "def compute_word2vec_vector(texts):\n",
    "    # print('\\n-----------------计算文本的Word2Vec向量表示-----------------')\n",
    "    tokenized_texts = [text.split() for text in texts]\n",
    "    # print('Tokenized Texts Length:', len(tokenized_texts))\n",
    "\n",
    "    model = Word2Vec(tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    vectors = [np.mean([model.wv[word] for word in text if word in model.wv] or [np.zeros(100)], axis=0) for text in tokenized_texts]\n",
    "    # print('Word2Vec Vectors Shape:', np.array(vectors).shape)\n",
    "    return vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算两个文本的Wasserstein距离\n",
    "def calculate_wasserstein_distance(vector1, vector2):\n",
    "    return wasserstein_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wasserstein 距离: 0.015600000000000003\n"
     ]
    }
   ],
   "source": [
    "u = [0.2329]\n",
    "v = [0.2485]\n",
    "\n",
    "distance = wasserstein_distance(u, v)\n",
    "\n",
    "print(f'Wasserstein 距离: {distance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 两两计算相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算两个文本的相似度\n",
    "def similarityCalculation_two_texts(file_path1, file_path2):\n",
    "    # 读取Word文档内容\n",
    "    text1 = read_word_doc(file_path1)\n",
    "    text2 = read_word_doc(file_path2)\n",
    "\n",
    "    # 计算TF-IDF向量表示\n",
    "    tfidf_vectors, _ = compute_tfidf_vector([text1, text2])\n",
    "\n",
    "    # 计算Word2Vec向量表示\n",
    "    word2vec_vectors = compute_word2vec_vector([text1, text2])\n",
    "    # print('Word2Vec Vectors:', word2vec_vectors)  # (2, 100)\n",
    "\n",
    "    # 用TF-IDF向量计算Wasserstein距离\n",
    "    distance_tfidf = calculate_wasserstein_distance(tfidf_vectors[0], tfidf_vectors[1])\n",
    "\n",
    "    # 用Word2Vec向量计算Wasserstein距离\n",
    "    distance_w2v = calculate_wasserstein_distance(word2vec_vectors[0], word2vec_vectors[1])\n",
    "\n",
    "    print(f'\\nThe Wasserstein distance based on tfidf between the two texts is: {distance_tfidf}')\n",
    "    print(f'\\nThe Wasserstein distance based on word2vector between the two texts is: {distance_w2v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------读取Word文档内容-----------------\n",
      "去除空格和标点符号前的文本长度： 3150\n",
      "去除空格和标点符号后的文本长度： 2828\n",
      "文本内容: 作为推动科技创新的重要引擎科技金融是中央金融工作会议提出的五篇大文章之一近日国家金融监督管理总局党委书记局长李云泽表示做好五篇大文章是金融业责无旁贷的重要使命银行保险机构要立足职责定位深入探索金融服务新质生产力的新路径新模式开发适配性强的金融产品提高金融服务水平全力支持科技创新绿色发展新兴产业和未来产业目前金融服务科创企业还存在哪些难点如何引导金融呵护科创企业发展加大信贷供给今年以来国家金融监督管理总局会同相关部门不断完善金融支持科技创新的政策丰富金融支持工具引导银行业加力服务科技型企业无论是信贷产品创新还是一揽子金融服务模式都旨在提升金融服务科创企业的针对性有效性为扎实做好科技金融大文章推动银行业加大科技型企业金融供给与服务中国人民银行金融监管总局等部门于近日联合印发关于扎实做好科技金融大文章的工作方案围绕培育支持科技创新的金融市场生态提出一系列有针对性的工作举措此外今年5月份金融监管总局发布关于银行业保险业做好金融五篇大文章的指导意见提出未来5年银行业保险业多层次广覆盖多样化可持续的五篇大文章服务体系基本形成相关工作机制更加完善产品更加丰富今年1月份金融监管总局发布关于加强科技型企业全生命周期金融服务的通知引导银行机构把更多金融资源用于促进科技创新不断提升金融支持科技型企业质效中国银行研究院研究员叶银丹表示通过一系列金融监管政策的引导和激励如税收优惠融资对接等措施为科创企业融资提供了良好的政策环境有助于吸引更多的金融资源投入科技创新领域增强金融支持精准性和可持续性加速推进科技成果转化和科创产业进程金融监管部门和银行机构深化金融供给侧结构性改革切实把五篇大文章落地落细提高金融服务实体经济的质量和水平2024年一季度金融机构贷款投向统计报告显示贷款支持科创企业力度较大2024年一季度末获得贷款支持的科技型中小企业2173万家获贷率479比上年末高11个百分点科技型中小企业本外币贷款余额27万亿元同比增长204比同期各项贷款增速高112个百分点兴业研究公司金融监管高级研究员陈昊表示对于金融机构自身而言在我国经济增长换挡的背景下银行业亟需寻找未来资产投放的蓄水池作为国家政策支持重点的科技创新领域需要大量融资支持在此背景下银行等金融机构有效拓展科技企业融资向科技创新领域注入大量资金不仅契合了国家政策导向也能够解决好金融机构所面临的息差承压等问题成为我国未来科技创新的重要动力瞄准融资痛点发展科技金融并非易事部分中小微科创企业时常面临融资难融资贵的难题银行传统贷款按期还本付息模式与科创企业融资偏好之间不匹配使得科创企业时常面临融资困境陈昊表示从传统的授信审批和贷款发放模式来看对于缺乏财务记录和信用记录的初创型企业而言银行往往会要求企业给予抵押物或由其他主体进行担保然而对于轻资产的科创企业而言其往往又缺乏银行普遍接受的不动产抵质押物由此传统的偏重抵质押担保或信用记录财务记录的银行授信和贷款模式难以契合科创企业的实际情况从科创企业自身特点分析来看叶银丹表示科创企业面临较高的技术风险和市场风险金融机构在风险识别和管理上存在挑战这使得银行和保险公司在提供融资时更为谨慎限制了科创企业的融资渠道额度科创企业在融资时往往依赖有限的几种传统金融产品导致融资结构单一难以满足短时间内的融资需求专家表示从多地破解融资痛点的角度来看近年来在监管部门的引导下银行机构采取了多种方式破解科创企业融资痛点尤其是打破银企之间存在的信息不对称问题在广西南宁市为了更好地支持科创企业发展当地银行机构与税务部门深化数据共享帮助科创企业依托纳税信用快速从银行机构融资实现税银企高效联动得益于银税互动平台的数据共享桂林银行南宁分行及时为我们授信250万元的信用贷款有效缓解了科研资金压力广西翔腾信息科技有限公司财务经理苏静媛表示近年来桂林银行南宁分行推出信用贷款知识产权质押等科创系列信贷产品精准清除科创小微企业融资路上缺少抵押资产的障碍全面提升金融服务科创企业的含金量叶银丹表示银行机构与科创企业之间要增强信息共享与透明度有助于银行机构准确评估科创企业的信用和创新能力为后续提供个性化的金融产品提供依据目前我国正在加快推进建立信息共享平台如全国一体化融资信用服务平台可以加强水电气社保公积金等信息的归集共享从而降低信息不对称性提高融资效率对银行来说还应加快金融产品和服务创新通过信贷股权债券承销等多种方式为科创企业提供金融支持细化多元服务信贷力量呵护科创企业成长不是一锤子买卖近年来银行机构不断探索金融支持科创的新路径逐渐形成科技型企业全生命周期金融服务即构建覆盖科技型企业从初创到成熟的全周期金融支持体系实现金融资源与科技需求的有效对接业内人士表示金融赋能科创的举措不能一成不变而应在结合企业不同成长阶段的基础上细分服务浙江农商联合银行辖内龙港农商银行相关负责人表示该行积极贯彻落实金融监管政策打好金融服务科创企业组合拳一是以一企一策差异化经营策略打造初创期成长期等阶段的小微企业专属金融服务方案二是打造多元化信贷产品加快科创指数知识产权质押等贷款投放为科创小微企业提供信贷支持三是将科创小微企业纳入网格化服务采取专职团队专人模式定点常态化走访对接帮助科创企业懂金融用金融值得注意的是金融服务科创也离不开良好的融资发展环境记者了解到多地有关部门和银行机构通过搭建多维度多层次的融资服务体系推动科技与金融双向奔赴在四川省成都市新都区政务服务管理和行政审批局积极搭建政银企服务平台架好企业与金融机构之间的桥梁促进科技金融支撑当地航空产业链上的科创企业发展我们为新都区航空科创中心的科创企业开展整体授信目前入驻该中心的成都承奥科技有限公司已获得200万元的航空贷后续将不断优化信贷投放机制更好满足科创企业金融需求中国银行新都支行客户经理吴询表示在浙江省嘉兴经济技术开发区塘汇街道与浙江禾城农商银行设立金融指导员助企机制对辖内制造业企业开展融资需求摸底等全流程金融服务今年以来该行还通过单列信贷规模推出专精特新企业专项惠企服务匹配差异化利率定价等方式加大科创企业服务力度不同阶段的科技型企业其所需要和适配的融资方案有较大的差异银行机构应探索更多举措服务科创招联首席研究员董希淼表示各级政府和金融管理部门应协同发力综合施策为金融服务科技型企业创造良好的制度环境推动科技金融健康发展更好地防范科技金融风险比如建立健全企业金融机构政府各方责任共担和损失分担机制加快出台相关标准包括科创企业认定标准认股权证业务标准等为金融机构服务科技型企业创造更好条件银行优化服务机制促进信贷资源精准赋能科创企业贷后管理也是做好科技金融服务不可或缺的一环叶银丹建议银行需建立严格的贷后跟踪和管理机制实时监控科创企业的财务状况经营活动和市场变化及时发现潜在风险并采取措施对于初创期和成长期的科技型企业银行应提供财务顾问服务辅助企业优化资金使用效率实现风险与收益的平衡促进科技型企业成长壮大\n",
      "\n",
      "-----------------读取Word文档内容-----------------\n",
      "去除空格和标点符号前的文本长度： 751\n",
      "去除空格和标点符号后的文本长度： 684\n",
      "文本内容: 科技金融探索服务新路径在当今快速发展的数字化时代科技金融正在成为推动经济发展的新动力随着大数据云计算人工智能等技术的不断革新科技金融领域正积极探索服务新路径以满足市场日益增长的金融需求近日多家科技金融公司纷纷推出创新产品和服务力求在传统金融服务的基础上提供更加便捷高效个性化的解决方案这些新路径不仅提升了金融服务的可达性和覆盖范围也为客户带来了前所未有的便利一方面科技金融通过大数据分析和人工智能技术实现了对用户信用状况的精准评估这种评估方式不仅提高了信贷决策的准确性和效率还降低了信贷风险同时也为那些传统金融体系中难以获得服务的群体提供了新的融资机会另一方面科技金融在服务流程上也进行了大胆的创新借助移动互联网和智能终端设备用户可以随时随地完成金融交易无需前往实体银行或金融机构此外智能语音助手和机器人客服的引入进一步提升了客户服务的自助化和智能化水平除了上述创新科技金融还在跨境支付数字货币智能投顾等领域进行了深入的探索这些新兴服务不仅为用户提供了更多元化的金融选择也为金融行业的未来发展指明了方向专家指出科技金融的快速发展得益于技术创新的推动和市场需求的拉动随着5G物联网等技术的不断成熟科技金融有望在未来实现更加广泛的应用和更深的渗透然而科技金融的迅猛发展也带来了一系列挑战包括数据安全隐私保护监管合规等问题业内人士呼吁相关部门应加强监管确保科技金融的健康发展同时行业自身也应加强自律提高风险防控能力展望未来科技金融将继续探索服务新路径助力实体经济为广大用户提供更加优质高效的金融服务随着技术的不断进步和市场的日益开放科技金融有望成为推动全球经济发展的重要力量\n",
      "\n",
      "-----------------计算文本的TF-IDF向量表示-----------------\n",
      "TF-IDF矩阵的形状： (2, 2198)\n",
      "打印特征名称： ['1' '1 月份' '11' ... '高级 研究员' '龙港' '龙港 农商']\n",
      "\n",
      "The Wasserstein distance based on tfidf between the two texts is: 0.007069184203715594\n",
      "\n",
      "The Wasserstein distance based on word2vector between the two texts is: 0.0007134504406167252\n"
     ]
    }
   ],
   "source": [
    "# main\n",
    "file_path1 = 'news\\财经\\科技金融探索服务新路径.docx'\n",
    "file_path2 = 'news_AI\\财经\\科技金融探索服务新路径.docx'\n",
    "\n",
    "similarityCalculation_two_texts(file_path1, file_path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量计算相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 遍历+打印文件夹数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_data(folder_path):\n",
    "    news_data = {}\n",
    "    \n",
    "    # 遍历文件夹中的每个子文件夹（新闻类型文件夹）\n",
    "    for news_type_folder in os.listdir(folder_path):\n",
    "        news_type_path = os.path.join(folder_path, news_type_folder)\n",
    "        if os.path.isdir(news_type_path):\n",
    "            # 初始化新闻类型对应的新闻列表\n",
    "            news_data[news_type_folder] = {}\n",
    "            \n",
    "            # 遍历每个新闻类型文件夹中的Word文件\n",
    "            for doc_file in os.listdir(news_type_path):\n",
    "                doc_file_path = os.path.join(news_type_path, doc_file)\n",
    "                if doc_file_path.endswith('.docx'):\n",
    "                    try:\n",
    "                        content = read_word_doc(doc_file_path)\n",
    "                        # 将新闻内容添加到新闻字典中\n",
    "                        news_data[news_type_folder][doc_file] = content\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {doc_file_path}: {e}\")\n",
    "    \n",
    "    return news_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'体育': {},\n",
      " '国际': {},\n",
      " '时政': {'大规模宕机为全球信息技术安全敲响警钟.docx': '大规模宕机为全球信息技术安全敲响警钟新华社伦敦7月20日电记者郭爽19日微软视窗系统以及该公司其他部分应用和服务发生大规模宕机造成多国航空铁路海运金融医疗酒店等行业无法正常运转众多企业和个人用户的工作和生活受到严重干扰微软首席执行官萨蒂亚纳德拉当天在社交媒体X上发文确认为微软提供服务的安全技术企业众击公司发布的一项软件更新是造成这次全球性宕机的主要原因这起宕机事件的影响范围和严重程度十分罕见为各国政府行业和个人用户敲响警钟英国工程技术学会的网络安全专家朱奈德阿里指出这次宕机的规模可能史无前例对全球信息技术IT行业团队构成了重大挑战但同时也为软件工程专业人员提供重要经验完全消除影响仍需时间据外媒报道总部位于美国的众击公司在全球拥有超过2万客户其中包括微软和亚马逊等科技巨头该公司首席执行官乔治库尔茨19日在社交媒体X上发文说此次事件不涉及网络攻击而是源自该公司为微软视窗系统发布的软件更新中存在缺陷该问题已被识别隔离并已部署修复措施库尔茨当天还在媒体采访中说就我们给客户旅行者以及所有受波及者造成的影响深表歉意公司正在努力解决问题但一些系统可能需要一些时间才能从此次故障中恢复过来虽然众击公司已经与微软合作迅速恢复大部分服务但专家认为需要进一步评估这次宕机事件的长期影响英国计算机学会网络安全专家亚当史密斯指出修复程序必须应用于世界各地的大量计算机这需要一段时间但如果计算机进入蓝屏和无限循环恢复可能会更困难需要几天甚至几周时间朱奈德阿里认为众击公司正将此事件列为最优先事项加以解决这次宕机的长期影响尚未被完全理解但它们将影响到未来关键安全更新的及时采用对IT系统风险保持警惕专家认为宕机事件凸显全球互联网基础设施的脆弱性需要对IT系统的复杂性以及各领域高度依赖网络基础设施的潜在风险保持警惕英国工程技术学会专家伊恩科登说世界各地发生的重大IT系统中断事件反映了经济国防和国家安全等方面对数字服务依赖日益增加的问题也因此凸显数字服务安全和韧性的重要性英国布里斯托尔大学计算机科学学院专家奥莫罗尼亚认为需要时刻警惕我们每天依赖的云基础设施和其他关键系统如今的网络基础设施非常复杂对其依赖性很广泛而对负责构建这些基础设施的人来说这些风险往往并不明显本次事件也存在大众尚不明了的复杂情况比如许多外媒提到微软视窗系统以及该公司其他部分应用和服务都出现问题有媒体援引微软发言人的话说微软365服务在7月18日夜间至19日出现的问题与众击公司的软件更新没有关系总体来说业内人士普遍认为微软视窗系统大范围宕机的原因是众击公司在软件更新中的失误有业内人士表示这表明企业在部署安全软件之前应彻底审查其网络安全解决方案的潜在风险数字安全企业IDEE创始人兼首席执行官阿尔拉卡尼在一份声明中指出这里的教训显而易见投资网络安全不仅是为了获得最新或最流行的工具还为了确保这些工具是可靠和有韧性的应急响应能力亟需提升此次事件影响波及全球也暴露出高度依赖IT系统的一些命脉行业及大型企业应急响应能力的不足例如全球航空业受到宕机的严重冲击美联社援引航班跟踪网站数据报道说截至美国东部时间19日傍晚全美近2800个航班被取消近1万个航班延误而全球约4400个航班被取消业内人士指出企业应建立健全网络故障应急响应计划定期进行演练以确保在故障发生时能够快速响应和恢复科登指出为了减轻网络故障的影响企业应装有备份系统留出基础设施的冗余度定期进行灾难恢复测试并制定严格的软件更新协议此外企业还应使用先进的监测工具就应对宕机等突发情况进行IT人员培训并与第三方供应商密切合作以确保制定有力的安全策略等澳大利亚国立大学计算机专家汤姆沃辛顿警告大范围宕机显示出依赖单一技术提供重要服务的风险应使用不同软件建立备用通信链路这确实增加了安全和维护成本但如果把所有鸡蛋都放在一个篮子里最终可能会丢脸'}}\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'news'  # 替换为你的文件夹路径\n",
    "news_data = get_news_data(folder_path)\n",
    "\n",
    "# 美观地打印嵌套字典结果\n",
    "import pprint\n",
    "pprint.pprint(news_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News type: 体育\n",
      "News type: 国际\n",
      "News type: 时政\n",
      "  - Document name: 大规模宕机为全球信息技术安全敲响警钟.docx\n"
     ]
    }
   ],
   "source": [
    "# 遍历并打印每种新闻类型文件夹中的Word文档名\n",
    "for news_type, docs in news_data.items():\n",
    "    print(f\"News type: {news_type}\")\n",
    "    for doc_file in docs.keys():\n",
    "        print(f\"  - Document name: {doc_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文心一言API调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_Key = 'ARTiXOfYDoIwrBhRo0FLRSoT'\n",
    "Secret_Key = 'BvHofSd0ENnmKkRID1IYR0lgbO1VP3aT'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"refresh_token\":\"25.c3c82ef1b303b11b6969bcd55922837c.315360000.2037018086.282335-81691303\",\"expires_in\":2592000,\"session_key\":\"9mzdA86edtC+hRM0cyd8X7ZQqNiLehwh7O4+ZGU8X8CTNRJUgQaX5ho\\/\\/LqSR7M\\/8FfitZ1UGCeFxZgEldad6O1zp1LGMw==\",\"access_token\":\"24.8d169516360fc523f7a6659dcc2486d7.2592000.1724250086.282335-81691303\",\"scope\":\"public brain_all_scope wenxinworkshop_mgr ai_custom_yiyan_com ai_custom_yiyan_com_eb_instant ai_custom_yiyan_com_bloomz7b1 ai_custom_yiyan_com_emb_text ai_custom_yiyan_com_llama2_7b ai_custom_yiyan_com_llama2_13b ai_custom_yiyan_com_llama2_70b ai_custom_yiyan_com_chatglm2_6b_32k ai_custom_yiyan_com_aquilachat_7b ai_custom_yiyan_com_emb_bge_large_zh ai_custom_yiyan_com_emb_bge_large_en ai_custom_yiyan_com_qianfan_chinese_llama_2_7b ai_custom_qianfan_bloomz_7b_compressed ai_custom_yiyan_com_eb_pro ai_custom_yiyan_com_adv_pro ai_custom_yiyan_com_sd_xl ai_custom_yiyan_com_tokenizer_eb ai_custom_yiyan_com_ai_apaas ai_custom_yiyan_com_qf_chinese_llama_2_13b ai_custom_yiyan_com_sqlcoder_7b ai_custom_yiyan_com_codellama_7b_ins ai_custom_yiyan_com_xuanyuan_70b_chat ai_custom_yiyan_com_yi_34b ai_custom_yiyan_com_chatlaw ai_custom_yiyan_com_emb_tao_8k ai_custom_yiyan_com_128k ai_custom_yiyan_com_eb_turbo_pro ai_custom_yiyan_com_mixtral_8x7b ai_custom_yiyan_com_prmtv ai_custom_yiyan_com_eb_pro_prmtv ai_custom_yiyan_com_eb_turbo_pro_128k ai_custom_yiyan_com_ernie_35_4k_0205 ai_custom_yiyan_com_ernie_35_8k_0205 ai_custom_yiyan_com_ernie_35_8k_1222 ai_custom_yiyan_com_ernie_lite_8k ai_custom_yiyan_com_gemma_7b_it ai_custom_yiyan_com_bce_reranker_base ai_custom_yiyan_com_fuyu_8b ai_custom_yiyan_com_ernie_tiny_8k ai_custom_yiyan_com_ernie_char_8k ai_custom_yiyan_com_ernie_35_8k_preview ai_custom_yiyan_com_ernie_40_8k_preview ai_custom_yiyan_com_ernie_func_8k ai_custom_yiyan_com_llama3_8b ai_custom_yiyan_com_llama3_70b ai_custom_yiyan_com_ernie_40_8k_0104 ai_custom_yiyan_com_ernie_40_8k_0329 ai_custom_yiyan_com_ernie_35_8k_0329 ai_custom_yiyan_com_qf_chinese_llama_2_70b ai_custom_yiyan_com_ernie_40_8k_beta ai_custom_yiyan_com_ernie_40_8k_0613 ai_custom_yiyan_com_ernie_35_8k_0613 ai_custom_yiyan_com_ernie_char_fiction_8k ai_custom_yiyan_com_ernie_40_turbo_8k(2) ai_custom_yiyan_com_ai_apaas_lite ai_custom_yiyan_com_ernie_40_turbo_8k_preview wise_adapt lebo_resource_base lightservice_public hetu_basic lightcms_map_poi kaidian_kaidian ApsMisTest_Test\\u6743\\u9650 vis-classify_flower lpq_\\u5f00\\u653e cop_helloScope ApsMis_fangdi_permission smartapp_snsapi_base smartapp_mapp_dev_manage iop_autocar oauth_tp_app smartapp_smart_game_openapi oauth_sessionkey smartapp_swanid_verify smartapp_opensource_openapi smartapp_opensource_recapi fake_face_detect_\\u5f00\\u653eScope vis-ocr_\\u865a\\u62df\\u4eba\\u7269\\u52a9\\u7406 idl-video_\\u865a\\u62df\\u4eba\\u7269\\u52a9\\u7406 smartapp_component smartapp_search_plugin avatar_video_test b2b_tp_openapi b2b_tp_openapi_online smartapp_gov_aladin_to_xcx\",\"session_secret\":\"34ed920e6803bfb7dc897575fb6d7e0a\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "def main():\n",
    "        \n",
    "    url = \"https://aip.baidubce.com/oauth/2.0/token?client_id=ARTiXOfYDoIwrBhRo0FLRSoT&client_secret=BvHofSd0ENnmKkRID1IYR0lgbO1VP3aT&grant_type=client_credentials\"\n",
    "    \n",
    "    payload = json.dumps(\"\")\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "    \n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    \n",
    "    print(response.text)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
